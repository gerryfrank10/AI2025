{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **How to start the lab:**\n",
    "\n",
    "1.\tDownload the python code from Brightspace.\n",
    "2.\tOpen the code in Google Colab.\n",
    "3.\tDownload the dataset and review the dataset.\n",
    "4.\tTasks are given with in the Python program.\n",
    "5.\tComplete each task.\n",
    "6.\tFinally, submit the file in Brightspace. (Please check Lab3 lab file to know how to submit)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='intro'></a>\n",
    "# **# Case Study: Income classification/Prediction Using Supervise Learning**\n",
    "\n",
    "# **You will start from the basics.**\n",
    "\n",
    "**The aim of the project** is to employ several supervised algorithms to accurately model individuals' income, whether he makes more than 50,000 or not, using data collected from the 1994 U.S. Census.\n",
    "\n",
    "The dataset that will be used is the **Census income dataset**, which was extracted from the machine learning repository (UCI), which contains about 32561 rows and 15 features. This dataset is large enough to understand supervised learning methods.\n",
    "\n",
    "You can download dataset using the UCI website as given below:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Census+Income\n",
    "\n",
    "# **The tasks are as follows:**\n",
    "\n",
    "1. Open UCI website then open Data Folder.\n",
    "2. Download \"adult.data\". This is the dataset we will process.This is raw data and we will not be able to process it in its current form. Therefore, we need to complete the next task.\n",
    "3. Perform data cleaning.\n",
    "4. Perform other tasks one by one.\n",
    "5. Prepare a report for your own record (report can be utilised to assignment preparation.)\n",
    "6. I highly recommend preparing report."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import zipfile\n",
    "\n",
    "# import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Task**: Read the dataset. You can download dataset using the UCI website as given below:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Census+Income"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "url = 'https://archive.ics.uci.edu/static/public/20/census+income.zip'\n",
    "with open('census_income.zip', 'wb') as f:\n",
    "    f.write(requests.get(url).content)\n",
    "\n",
    "column_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "       'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "       'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n",
    "\n",
    "with zipfile.ZipFile('census_income.zip', 'r') as zip_ref:\n",
    "    with zip_ref.open('adult.data') as file:\n",
    "        cens = pd.read_csv(file, names=column_names, header=None, na_values='?')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The project poses some questions about this experiment:\n",
    "\n",
    "**1. Does an individual make more than 50k income or not?**\n",
    "\n",
    "**2. What are the most important features that help to define the income of an individual?**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='overview'></a>\n",
    "# Overview"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# take an overview look at the data\n",
    "cens.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Task: Display first 20 instances of the dataset."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Write code here\n",
    "cens.head(20) # Display the 20 instances of the dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cens.info()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Total number of records\n",
    "n_records = cens.shape[0]\n",
    "\n",
    "# Total number of features\n",
    "n_features = cens.shape[1]\n",
    "\n",
    "# Number of records where individual's income is more than $50,000\n",
    "n_greater_50k = cens[cens['income'] == ' <=50K'].shape[0]\n",
    "\n",
    "# Number of records where individual's income is at most $50,000\n",
    "n_at_most_50k = cens[cens['income'] == ' >50K'].shape[0]\n",
    "\n",
    "# Percentage of individuals whose income is more than $50,000\n",
    "greater_percent =  (n_greater_50k / n_records) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"Total number of records: {}\".format(n_records))\n",
    "print(\"Total number of features: {}\".format(n_features))\n",
    "print(\"Individuals making more than $50k: {}\".format(n_greater_50k))\n",
    "print(\"Individuals making at most $50k: {}\".format(n_at_most_50k))\n",
    "print(\"Percentage of individuals making more than $50k: {:.2f}%\".format(greater_percent))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='clean'></a>\n",
    "# Data Cleaning"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# drop unneeded columns\n",
    "cens.drop('education', inplace=True, axis=1)\n",
    "cens.columns.tolist()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- We have dropped the education feature. Which is a duplicate feature of education_num, but in a nonnumerical format.\n",
    "\n",
    "The matching education level of the education number:\n",
    "\n",
    "**1**: Preschool, **2**: 1st-4th, **3**: 5th-6th, **4**: 7th-8th, **5**: 9th, **6**: 10th, **7**: 11th, **8**: 12th, **9**: HS-grad,\n",
    "\n",
    "**10**: Some-college, **11**: Assoc-voc, **12**: Assoc-acdm, **13**: Bachelors, **14**: Masters, **15**: Prof-school, **16**: Doctorate"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check for nulls\n",
    "cens.isna().sum()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- It appears that there are no null values occurred in the dataset."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# check duplicates and remove it\n",
    "print(\"Before removing duplicates:\", cens.duplicated().sum())\n",
    "\n",
    "cens = cens[~cens.duplicated()]\n",
    "\n",
    "print(\"After removing duplicates:\", cens.duplicated().sum())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- There are 24 duplicate rows in our dataset. So, we remove them to make the data more realistic and free-error."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before discarding\n",
    "cens['sex'].value_counts()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# discard spaces from entries\n",
    "columns = ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country', 'income']\n",
    "# for column in columns:\n",
    "#     cens[column] = cens[column].str.strip()\n",
    "# Same as above but with list comprehension from CHATGPT\n",
    "cens[[col for col in columns]] = cens[[col for col in columns]].apply(lambda x: x.str.strip())\n",
    "cens.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after discarding\n",
    "cens.sex.value_counts()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Discarding the spaces from the entries of the dataset, for easier access."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before changing \"?\"\n",
    "cens.workclass.value_counts()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# changing \"?\" to Unknown\n",
    "change_columns = ['workclass', 'occupation', 'native_country']\n",
    "# for column in change_columns:\n",
    "#         cens[column] = cens[column].replace({'?': 'Unknown'})\n",
    "\n",
    "# Again same with above but my code.\n",
    "cens[[col for col in columns]] = cens[[col for col in columns]].apply(lambda x: x.replace({'?':'Unknown'}))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after changing \"?\"\n",
    "cens.workclass.value_counts()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Looking through the dataset again\n",
    "cens.head()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Changing \"?\" symbol to \"Unknown\", for better interpretation and cleaner representation."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='explore'></a>\n",
    "# Data Exploration"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# a quick look on some statistics about the data\n",
    "cens.describe()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ct_counts = cens.groupby(['education_num', 'income']).agg(count=('income', 'size'))\n",
    "ct_counts = ct_counts.reset_index()\n",
    "ct_counts\n",
    "# ct_counts = cens.groupby(['education_num', 'income']).size()\n",
    "# ct_counts.head()\n",
    "# ct_transform = cens.groupby(['education_num']).transform(lambda x: x.nunique())\n",
    "# ct_transform"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ct_counts = ct_counts.pivot(index='education_num', columns='income', values='count').fillna(0)\n",
    "ct_counts"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(ct_counts, annot=True, fmt='.0f', cbar_kws={'label': 'Number of Individuals'})\n",
    "plt.title('Number of People for Education class relative to column')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Education class');"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Heat map\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "ct_counts = cens.groupby(['education_num', 'income']).size()\n",
    "ct_counts = ct_counts.reset_index(name = 'count')\n",
    "ct_counts = ct_counts.pivot(index = 'education_num', columns = 'income', values = 'count').fillna(0)\n",
    "\n",
    "sb.heatmap(ct_counts, annot = True, fmt = '.0f', cbar_kws = {'label' : 'Number of Individuals'})\n",
    "plt.title('Number of People for Education Class relative to Income')\n",
    "plt.xlabel('Income ($)')\n",
    "plt.ylabel('Education Class');"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- In the graph above, we can see that people with education classes of 9 & 10 make up the highest portion in the dataset. Also, we notice that people with education class of 14 to 16 proportionally usually make >50k as income in the statistics we have in the dataset, unlike lesser education classes where they usually make <=50k as income."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clustered Bar Chart\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sb.barplot(data = cens, x = 'income', y = 'age', hue = 'sex')\n",
    "ax.legend(loc = 8, ncol = 3, framealpha = 1, title = 'Sex')\n",
    "plt.title('Average of Age for Sex relative to Income')\n",
    "plt.xlabel('Income ($)')\n",
    "plt.ylabel('Average of Age');"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- The figure shows in general that the people with >50K has a higher average age than the ones with <=50K. And in both cases of income, we see that the male category has a little bit greater age average than the female category."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Bar Chart\n",
    "plt.figure(figsize=(8,6))\n",
    "sb.barplot(data=cens, x='income', y='hours_per_week', palette='YlGnBu')\n",
    "plt.title('Average of Hours per Week relative to Income')\n",
    "plt.xlabel('Income ($)')\n",
    "plt.ylabel('Average of Hours per Week');"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- We notice here that the income grows directly with the average of work hours per week, which is a pretty reasonable and logical result."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='preprocess'></a>\n",
    "# Data Preprocessing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cens_prep = cens.copy()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- We have taken a copy of the dataset to maintain the cleaned one for later uses, and to use the copied one for preparing the data for the model."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "numerical = ['age', 'capital_gain', 'capital_loss', 'hours_per_week', 'fnlwgt']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "cens_prep[numerical] = scaler.fit_transform(cens_prep[numerical])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cens_prep.sample(3)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The data has been scaled to MinMaxScalling for numerical features, which converts the data to have a range between 0 and 1. That would help to make the data well-prepared for the model.\n",
    "\n",
    "$$\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encoding\n",
    "cens_prep['sex'] = cens_prep.sex.replace({\"Female\": 0, \"Male\": 1})\n",
    "cens_prep['income'] = cens_prep.income.replace({\"<=50K\": 0, \">50K\": 1})\n",
    "\n",
    "# Create dummy variables\n",
    "cens_prep = pd.get_dummies(cens_prep)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "encoded = list(cens_prep.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- We have encoded and created dummy variables using the hot-encoding approach for the categorical features, to make it as numerical data. It helps for easier processing and more numerical representation."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='exp'></a>\n",
    "# Experimental Process"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the project, the **independent variables** have been chosen as follows:\n",
    "1. **Age**\n",
    "2. **Workclass**\n",
    "3. **Fnlwgt**\n",
    "4. **Education_num**\n",
    "5. **Marital_status**\n",
    "6. **Occupation**\n",
    "7. **Relationship**\n",
    "8. **Race**\n",
    "9. **Sex**\n",
    "10. **Capital_gain**\n",
    "11. **Capital_loss**\n",
    "12. **Hours_per_week**\n",
    "13. **Native_country**\n",
    "\n",
    "Also, the **Income** variable is considered to be the **dependent variable**, since it is our concern in this experiment."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Task: Analyse the code and import the libraries required to run this program**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import some classification models\n",
    "#Task 1: Import libraries for Random forest classifier, AdaBoost Classifier, Logistic Regression Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import needed functions\n",
    "#Task 2: Import libraries for cross validation, accuracy score, F1 score and splitting dataset.\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Task: Write code for splitting to training and testing. Training data = 80% and apply random split**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Portioning the data\n",
    "X = cens_prep.drop('income', axis=1)\n",
    "y = cens_prep['income']\n",
    "\n",
    "# Task-3: Write code for splitting to training and testing. Training data = 80% and apply random split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "models = {}\n",
    "\n",
    "# models with default parameter\n",
    "models['LR'] = LogisticRegression() # LG: Logic Rrednwi\n",
    "models['RandomForest'] = RandomForestClassifier() #modleik4gi34r\n",
    "models['AdaBoost'] = AdaBoostClassifier() #ejfreuifhg\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task: Write a short description on:\n",
    "1. Cross validation\n",
    "2. F1 Score\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Cross validation\n",
    "for model_name in models:   # a loop to choose the model defined in the above code\n",
    "    model = models[model_name]\n",
    "    results = cross_validate(model, X, y, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "\n",
    "    print(model_name + \":\")\n",
    "    print(\"Accuracy:\" , 'train: ', results['train_accuracy'].mean(), '| test: ', results['test_accuracy'].mean())\n",
    "    print(\"F1-score:\" , 'train: ', results['train_f1'].mean(), '| test: ', results['test_f1'].mean())\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- As it appears from the exploration in our dataset that there is an imbalance between the classes of classifications. Since the individuals making more than 50k as income represent 75% of the data. So, we would try to make oversampling."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "results = cross_validate(clf, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "print(\"Accuracy:\" , 'train: ', results['train_accuracy'].mean(), '| test: ', results['test_accuracy'].mean())\n",
    "print(\"F1-score:\" , 'train: ', results['train_f1'].mean(), '| test: ', results['test_f1'].mean())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# AdaBoost classifier\n",
    "ada = AdaBoostClassifier()\n",
    "results = cross_validate(ada, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "print(f'Accuracy: {results['train_accuracy'].mean()}, test accuracy: {results['test_accuracy'].mean()}')\n",
    "print(f'F1-score: {results['train_f1'].mean()}, test f1: {results['test_f1'].mean()}')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lgr = LogisticRegression()\n",
    "results = cross_validate(lgr, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "print(f'Accuracy: {results['train_accuracy'].mean()}, test accuracy: {results['test_accuracy'].mean()}')\n",
    "print(f'F1-score: {results['train_f1'].mean()}, test f1: {results['test_f1'].mean()}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Models Definitions:\n",
    "**Logistic regression**, despite its name, is a linear model for classification rather than regression. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "**A Random forest** is a meta estimator that fits several decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "**An AdaBoost classifier** is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluation Methodology:\n",
    "The data has been split into training and testing parts of the features and the label with a test size of 20% and with a random state to get the same randomness with the next runs. This happened by using the train_test_split function.\n",
    "\n",
    "Cross-validation has been applied between the models to select the most suitable ones, We have done that using the cross_validate function with 5 folds splitting. And outputs the train and test score of the model.\n",
    "\n",
    "All of This has been done that with a cleaned state of the data, also with the scaled and encoded version of it.\n",
    "\n",
    "Due to the fact that there is an imbalance in the classes of classification. If this has been fixed, it would help the model to learn better from the various classes and to not be biassed towards one over another.\n",
    "\n",
    "One way to fight this issue is to generate new samples in the classes which are under-represented (minority class). The most naive strategy is to generate new samples by randomly sampling with replacement of the currently available samples. This is called Oversampling, it is a technique used to modify unequal data classes to create balanced data sets.\n",
    "\n",
    "And that has been applied using RandomOverSampler class from the imblearn library, to generate the new resampled data.\n",
    "\n",
    "### Metrics used for Evaluation:\n",
    "\n",
    "We have used the accuracy metric for the evaluation of the models. We can describe the accuracy metric as the ratio between the number of correct predictions and the total number of predictions:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
    "\n",
    "Also, for binary classification, accuracy can also be calculated in terms of the confusion matrix terminology:\n",
    "\n",
    "$$\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
    "\n",
    "Where TP = True Positives, TN = True Negatives, FP = False Positives, and FN = False Negatives.\n",
    "\n",
    "\n",
    "Also, F1-score has been used as one of the metrics in the experiment, it can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.\n",
    "\n",
    "The formula for the F1 score is:\n",
    "\n",
    "$$\\text{F1} = \\frac{2 * (precision * recall)}{(precision + recall)}$$"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a id='conclude'></a>\n",
    "# Conclusions"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Features Importance"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from sklearn.preprocessing import LabelEncoder"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cens_conc = cens.copy()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for col in cens_conc.columns:\n",
    "    if cens_conc[col].dtypes == 'object':\n",
    "        encoder = LabelEncoder()\n",
    "        cens_conc[col] = encoder.fit_transform(cens_conc[col])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Portioning the data\n",
    "Xc = cens_conc.drop('income', axis=1)\n",
    "yc = cens_conc['income']\n",
    "\n",
    "# Splitting to training and testing\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=42)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(Xc_train, yc_train)\n",
    "\n",
    "\n",
    "# View a list of the features and their importance scores\n",
    "print('\\nFeatures Importance:')\n",
    "feat_imp = pd.DataFrame(zip(Xc.columns.tolist(), rfc.feature_importances_ * 100), columns=['feature', 'importance'])\n",
    "feat_imp"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task: Why one should perform 'Feature Importance'?\n",
    "\n",
    "Feature Importance is a way to measure how a variable contributes to a machine learning model's prediction, Feature importance works using techniques like decision trees,\n",
    "neural networks.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Features importance plot\n",
    "plt.figure(figsize=(20,6))\n",
    "sb.barplot(data=feat_imp, x='feature', y='importance')\n",
    "plt.title('Features Importance', weight='bold', fontsize=20)\n",
    "plt.xlabel('Feature', weight='bold', fontsize=13)\n",
    "plt.ylabel('Importance (%)', weight='bold', fontsize=13);\n",
    "\n",
    "\n",
    "# add annotations\n",
    "impo = feat_imp['importance']\n",
    "locs, labels = plt.xticks()\n",
    "\n",
    "for loc, label in zip(locs, labels):\n",
    "    count = impo[loc]\n",
    "    pct_string = '{:0.2f}%'.format(count)\n",
    "\n",
    "    plt.text(loc, count-0.8, pct_string, ha = 'center', color = 'w', weight='bold')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- We plan to drop the features that have less than 4% impartance, to speed up the process of fitting the model. Since without them, it would provide the same results of the evaluation."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Selection"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cens_final = cens.copy()"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cens_final.head(2)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "cens_final.drop(['race', 'sex', 'capital_loss', 'native_country'], axis=1, inplace=True)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Scaling\n",
    "numerical = ['age', 'capital_gain', 'hours_per_week', 'fnlwgt']\n",
    "scaler = MinMaxScaler()\n",
    "cens_final[numerical] = scaler.fit_transform(cens_final[numerical])\n",
    "\n",
    "# Encoding\n",
    "cens_final['income'] = cens_final.income.replace({\"<=50K\": 0, \">50K\": 1})\n",
    "\n",
    "# Create dummy variables\n",
    "cens_final = pd.get_dummies(cens_final)\n",
    "\n",
    "# Portioning\n",
    "Xf = cens_final.drop('income', axis=1)\n",
    "yf = cens_final['income']\n",
    "\n",
    "# Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(Xf, yf)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf2 = RandomForestClassifier()\n",
    "\n",
    "results = cross_validate(rf2, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "print(\"Accuracy:\" , 'train: ', results['train_accuracy'].mean(), '| test: ', results['test_accuracy'].mean())\n",
    "print(\"F1-score:\" , 'train: ', results['train_f1'].mean(), '| test: ', results['test_f1'].mean())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task: Analyse the code of random forset classifier and write your code for other two classifiers that adaboost and logistic regression.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "clf = RandomForestClassifier()\n",
    "\n",
    "results = cross_validate(clf, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "print(\"Accuracy:\" , 'train: ', results['train_accuracy'].mean(), '| test: ', results['test_accuracy'].mean())\n",
    "print(\"F1-score:\" , 'train: ', results['train_f1'].mean(), '| test: ', results['test_f1'].mean())"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lgr = LogisticRegression()\n",
    "results = cross_validate(lgr, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "print(f'Accuracy: {results['train_accuracy'].mean()}, test accuracy: {results['test_accuracy'].mean()}')\n",
    "print(f'F1-score: {results['train_f1'].mean()}, test f1: {results['test_f1'].mean()}')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Challenging Task: Try to implement to solve this problem using SVM and KNN."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Models definition\n",
    "knn = KNeighborsClassifier()\n",
    "svm = SVC(gamma='auto')\n",
    "\n",
    "# KNN model\n",
    "results = cross_validate(knn, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "print(f'------------- KNN Classifier -----------------')\n",
    "print(f'Accuracy: {results['train_accuracy'].mean()}, test accuracy: {results['test_accuracy'].mean()}')\n",
    "print(f'F1-score: {results['train_f1'].mean()}, test f1: {results['test_f1'].mean()}')\n",
    "\n",
    "# SVM model\n",
    "results = cross_validate(svm, X_resampled, y_resampled, cv=5, scoring=['accuracy', 'f1'], return_train_score=True)\n",
    "print(f'------------- SVM Classifier -----------------')\n",
    "print(f'Accuracy: {results['train_accuracy'].mean()}, test accuracy: {results['test_accuracy'].mean()}')\n",
    "print(f'F1-score: {results['train_f1'].mean()}, test f1: {results['test_f1'].mean()}')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
