{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerryfrank10/AI2025/blob/main/AI_Lab_08_ANN_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "capm-_wBlYIS"
      },
      "source": [
        "# AI Lab 09\n",
        "\n",
        "Data Transformation and MLPs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwav1BESlYIU"
      },
      "source": [
        "## Loading the dataset\n",
        "\n",
        "The first cell below loads the ``wine dataset``.\n",
        "\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
        "\n",
        "The second cell prints out the description of this dataset. In short, it is for **classification of wines** based on features like alcohol, magnesium and colour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:39.776859Z",
          "start_time": "2022-11-24T17:10:38.234261Z"
        },
        "id": "t5P0NmiflYIW"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Loading the dataset\n",
        "dataset = load_wine()\n",
        "\n",
        "# Get the X (feature matrix) and y (class label vector) from the data\n",
        "X, y = dataset.data, dataset.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDUf2XJtlYIX",
        "outputId": "5a352302-749b-411e-987a-c5145c64add3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".. _wine_dataset:\n",
            "\n",
            "Wine recognition dataset\n",
            "------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 178\n",
            "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
            "    :Attribute Information:\n",
            " \t\t- Alcohol\n",
            " \t\t- Malic acid\n",
            " \t\t- Ash\n",
            "\t\t- Alcalinity of ash  \n",
            " \t\t- Magnesium\n",
            "\t\t- Total phenols\n",
            " \t\t- Flavanoids\n",
            " \t\t- Nonflavanoid phenols\n",
            " \t\t- Proanthocyanins\n",
            "\t\t- Color intensity\n",
            " \t\t- Hue\n",
            " \t\t- OD280/OD315 of diluted wines\n",
            " \t\t- Proline\n",
            "\n",
            "    - class:\n",
            "            - class_0\n",
            "            - class_1\n",
            "            - class_2\n",
            "\t\t\n",
            "    :Summary Statistics:\n",
            "    \n",
            "    ============================= ==== ===== ======= =====\n",
            "                                   Min   Max   Mean     SD\n",
            "    ============================= ==== ===== ======= =====\n",
            "    Alcohol:                      11.0  14.8    13.0   0.8\n",
            "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
            "    Ash:                          1.36  3.23    2.36  0.27\n",
            "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
            "    Magnesium:                    70.0 162.0    99.7  14.3\n",
            "    Total Phenols:                0.98  3.88    2.29  0.63\n",
            "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
            "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
            "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
            "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
            "    Hue:                          0.48  1.71    0.96  0.23\n",
            "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
            "    Proline:                       278  1680     746   315\n",
            "    ============================= ==== ===== ======= =====\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
            "    :Creator: R.A. Fisher\n",
            "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
            "    :Date: July, 1988\n",
            "\n",
            "This is a copy of UCI ML Wine recognition datasets.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
            "\n",
            "The data is the results of a chemical analysis of wines grown in the same\n",
            "region in Italy by three different cultivators. There are thirteen different\n",
            "measurements taken for different constituents found in the three types of\n",
            "wine.\n",
            "\n",
            "Original Owners: \n",
            "\n",
            "Forina, M. et al, PARVUS - \n",
            "An Extendible Package for Data Exploration, Classification and Correlation. \n",
            "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
            "Via Brigata Salerno, 16147 Genoa, Italy.\n",
            "\n",
            "Citation:\n",
            "\n",
            "Lichman, M. (2013). UCI Machine Learning Repository\n",
            "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
            "School of Information and Computer Science. \n",
            "\n",
            "|details-start|\n",
            "**References**\n",
            "|details-split|\n",
            "\n",
            "(1) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "Comparison of Classifiers in High Dimensional Settings, \n",
            "Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
            "Mathematics and Statistics, James Cook University of North Queensland. \n",
            "(Also submitted to Technometrics). \n",
            "\n",
            "The data was used with many others for comparing various \n",
            "classifiers. The classes are separable, though only RDA \n",
            "has achieved 100% correct classification. \n",
            "(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
            "(All results using the leave-one-out technique) \n",
            "\n",
            "(2) S. Aeberhard, D. Coomans and O. de Vel, \n",
            "\"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
            "Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
            "Mathematics and Statistics, James Cook University of North Queensland. \n",
            "(Also submitted to Journal of Chemometrics).\n",
            "\n",
            "|details-end|\n"
          ]
        }
      ],
      "source": [
        "# Print out the dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSPSRMI8lYIX"
      },
      "source": [
        "## Scaling datasets\n",
        "\n",
        "Setting up 2 x feature matrices here, using the ``Normalizer`` and ``StandardScaler``, to experiment below on whether this helps the MLP perform better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:41.282576Z",
          "start_time": "2022-11-24T17:10:41.278257Z"
        },
        "id": "O-3W48WmlYIY"
      },
      "outputs": [],
      "source": [
        "# Normalising feature matrix values\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normalizer = Normalizer()\n",
        "normalizer.fit(X)\n",
        "X_normalised = normalizer.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:41.297659Z",
          "start_time": "2022-11-24T17:10:41.285121Z"
        },
        "id": "BcVcV9-FlYIY"
      },
      "outputs": [],
      "source": [
        "# Scaling feature matrix values\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_scaled = scaler.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hp9newWlYIY"
      },
      "source": [
        "## MLP validation\n",
        "\n",
        "The 1st cell below does the following:\n",
        "\n",
        " * Splits the dataset for training and testing (using the original feature values ``X``)\n",
        " * Creates an 3-layer ``MLPClassifier`` with 10 neurons in the hidden layer, to be trained for 100 epocs (iterations)\n",
        " * Trains the MLP and tests it\n",
        " * Calculates and prints out a confusion matrix and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-_bhLeHlYIY"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn import metrics\n",
        "\n",
        "def print_results(scores):\n",
        "    print(\"Accuracy:          %0.2f (+/- %0.2f)\" % (scores['test_score'].mean(), scores['test_score'].std() * 2))\n",
        "    print(\"Training time (s): %0.2f (+/- %0.2f)\" % (scores['fit_time'].mean(), scores['fit_time'].std() * 2))\n",
        "    print(\"Testing time (s):  %0.2f (+/- %0.2f)\" % (scores['score_time'].mean(), scores['score_time'].std() * 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1NYN6yBlYIY",
        "outputId": "958f6425-29fa-4d43-84ac-6133446352f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:          0.79 (+/- 0.24)\n",
            "Training time (s): 0.50 (+/- 1.09)\n",
            "Testing time (s):  0.00 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "# Instantiating MLP\n",
        "model = MLPClassifier(hidden_layer_sizes=(1000), max_iter=5000)\n",
        "\n",
        "# Validating MLP model\n",
        "scores = cross_validate(model, X, y, cv=5)\n",
        "\n",
        "# Printing performance results\n",
        "print_results(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts5U99pclYIY"
      },
      "source": [
        "You should have seen warnings about the MLP not having converged above (with the starting values), and a rather sub-optimal performance!\n",
        "\n",
        "**QUESTION**: after increasing ``max_iter`` to the point the convergance warning disappears, what's the performance like? Is it good enough?\n",
        "\n",
        "\n",
        "**SUGGESTED ANSWER**: you should note that even with 1000 neurons in the hidden layer and training for 5000 epochs, the performance is still quite poor (79% accuracy in the run cached in this notebook)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0SgLhRylYIY"
      },
      "source": [
        "## Normalised vs Scaled feature values\n",
        "\n",
        "If you tweak the number of neurons in the hidden layer and the maximum number of iterations (and other hyper-parameters), you will probably find that the performance remains quite poor. **IDEED, AS NOTED ABOVE**\n",
        "\n",
        "So, let us now move on to comparing the performance when using the normalised and scaled feature matrices instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NScRquWJlYIZ"
      },
      "source": [
        "### Normalised feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:48.052080Z",
          "start_time": "2022-11-24T17:10:42.201072Z"
        },
        "id": "a20mgPR-lYIZ",
        "outputId": "9710baf0-0bdd-4739-9118-b7cd2aef07b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:          0.91 (+/- 0.12)\n",
            "Training time (s): 0.40 (+/- 0.07)\n",
            "Testing time (s):  0.00 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "# Instantiating MLP\n",
        "model = MLPClassifier(hidden_layer_sizes=(100), max_iter=5000)\n",
        "\n",
        "# Validating MLP model\n",
        "scores = cross_validate(model, X_normalised, y, cv=5)\n",
        "\n",
        "# Printing performance results\n",
        "print_results(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixZJXGwWlYIZ"
      },
      "source": [
        "### Scaled feature matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-11-24T17:10:48.916816Z",
          "start_time": "2022-11-24T17:10:48.054587Z"
        },
        "id": "o5B-9Fv3lYIZ",
        "outputId": "26981af1-cec8-4088-d9b1-4059fb0b91b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:          0.96 (+/- 0.04)\n",
            "Training time (s): 0.07 (+/- 0.03)\n",
            "Testing time (s):  0.00 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "# Instantiating MLP\n",
        "model = MLPClassifier(hidden_layer_sizes=(10), max_iter=1000)\n",
        "\n",
        "# Validating MLP model\n",
        "scores = cross_validate(model, X_scaled, y, cv=5)\n",
        "\n",
        "# Printing performance results\n",
        "print_results(scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TlBcc5nlYIZ"
      },
      "source": [
        "## Discussion and Conclusions\n",
        "\n",
        "Above, you should be able to make a few key observations regarding:\n",
        "\n",
        "* The performance of MLPs in general on this dataset\n",
        "* How the performance is affected by\n",
        "  - Hyper-parameters like the number of neurons and number of epochs\n",
        "  - Data processing: original feature values vs normalised vs scaled\n",
        "\n",
        "What seems to be best?\n",
        "\n",
        "What seems to be worst?\n",
        "\n",
        "What seems to make the biggest difference to the performance?\n",
        "\n",
        "**SUGGESTED ANSWER / OBSERVATIONS**\n",
        "\n",
        "* Using the StandardScaler seems to be the best; 96% accuracy vs 91% vs 79%.\n",
        "* The Normalizer seems better than using the raw/original feature values.\n",
        "* HOWEVER, note how the performance with the Normalized feature values only reaches > 90% accuracy with a significanlty larger network; 100 neurons in the hidden layer vs 10 when using the StandardScaler. Also, the number of traing iterations had to be increased significantly.\n",
        "* Training time in the cached results are 0.07 seconds to train using feature values from the StandardScaler vs 0.40 seconds when using Normalized feature values.\n",
        "* It seems the biggest impact on performance is the data processing through scaling the feature values.\n",
        "* Performance may still improve with different hyper-parameters, which the bonus/extra materials below will explore.\n",
        "\n",
        "\n",
        "\n",
        "**PS**: Feel free to play around with other hyper-parameters as well, which you can see in the API reference documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzKkfePFlYIZ"
      },
      "source": [
        "## Bonus / Extras\n",
        "\n",
        "### 4-Layer MLP with more hyper-parameters\n",
        "\n",
        "Showing the use of more hyper-parameters and multiple hidden layers.\n",
        "\n",
        "**NOTE**: In Scikit-Learn, you can only set the activation function for neurons in the hidden layer (default ``ReLu``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs62_SINlYIZ",
        "outputId": "b54ccea0-fe6b-4708-ec35-f29a17a7c064"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:          0.98 (+/- 0.04)\n",
            "Training time (s): 0.10 (+/- 0.01)\n",
            "Testing time (s):  0.00 (+/- 0.00)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Scaling feature matrix\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "# Instantiating MLP\n",
        "model = MLPClassifier(hidden_layer_sizes=(5,5),\n",
        "                      activation='tanh',\n",
        "                      learning_rate='adaptive',\n",
        "                      max_iter=1000)\n",
        "\n",
        "# Validating MLP model\n",
        "scores = cross_validate(model, X_scaled, y, cv=5)\n",
        "\n",
        "# Printing performance results\n",
        "print_results(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T__y1sc-lYIZ"
      },
      "source": [
        "### Hyper-parameter optimisation\n",
        "\n",
        "Here's an example of using Random Search for automatically finding \"optimal\" hyper-parameters for an MLP.\n",
        "\n",
        "API Reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
        "\n",
        "Another popular hyper-parameter optimisation algorithm is Grid Search, which does a brute-force search, trying every single combination of hyper-parameter values. But this can be very time-consuming. Since the code would essentially be the same, only showing an example with Random Search here.\n",
        "\n",
        "This example has been modified from: https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGl3nHMvlYIZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils._testing import ignore_warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import numpy as np\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, rank_metric='score', n_top=3):\n",
        "    \"\"\"\n",
        "    Utility function to report best scores.\n",
        "    :param results: the cv_results_ data structure from the optimisation algorithm\n",
        "    :param rank_metric: name of the metric to report results for\n",
        "    :param n_top: the number of top results to report\n",
        "    \"\"\"\n",
        "    print(\"\\nModels ranked according to\", rank_metric)\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results[\"rank_test_\" + rank_metric] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.2f} (+/- {1:.2f})\".format(\n",
        "                  results[\"mean_test_\" + rank_metric][candidate],\n",
        "                  results[\"std_test_\" + rank_metric][candidate]*2))\n",
        "            print(\"Params: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "@ignore_warnings(category=ConvergenceWarning)\n",
        "def run_random_search(model, X, y, param_dict, num_itr=100):\n",
        "    \"\"\"\n",
        "    Note: you don't need to put random search code into such a function, but it is one way to suppress\n",
        "    the \"convergence warning spam\" you will get otherwise with models like the MLP.\n",
        "    \"\"\"\n",
        "\n",
        "    random_search = RandomizedSearchCV(model, # the MLP model\n",
        "                                       param_distributions=param_dict, # the dictionary of hyper-parameters and value space\n",
        "                                       n_iter=num_itr, # will try 100 random combinations of the above values\n",
        "                                       cv=5) # 5-fold cross-validation\n",
        "\n",
        "    random_search.fit(X, y)\n",
        "\n",
        "    return random_search.cv_results_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7p7Q0NjlYIa",
        "outputId": "e5acfae0-2473-4800-81ff-8e7197f4690b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> STARTING RANDOM SEARCH ...\n",
            "> RANDOM SEARCH COMPLETE\n",
            "\n",
            "RandomizedSearchCV took 14.80 seconds for 100 candidates parameter settings.\n",
            "\n",
            "Models ranked according to score\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.96 (+/- 0.08)\n",
            "Params: {'max_iter': 2000, 'learning_rate': 'constant', 'hidden_layer_sizes': 100, 'activation': 'logistic'}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.96 (+/- 0.09)\n",
            "Params: {'max_iter': 1000, 'learning_rate': 'constant', 'hidden_layer_sizes': 50, 'activation': 'logistic'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.96 (+/- 0.08)\n",
            "Params: {'max_iter': 2000, 'learning_rate': 'constant', 'hidden_layer_sizes': (20, 20), 'activation': 'tanh'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from time import time\n",
        "\n",
        "# instantiating the model (not setting any hyper-parameters here)\n",
        "model = model = MLPClassifier()\n",
        "\n",
        "# specify parameters and potential values to sample from\n",
        "param_dict = {\n",
        "    \"hidden_layer_sizes\": [5, 10, 20, 30, 50, 100, (5,5), (10,10), (20,20)],\n",
        "    \"max_iter\": [100, 300, 500, 1000, 2000],\n",
        "    \"activation\": [\"tanh\", \"relu\", \"logistic\", \"identity\"],\n",
        "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"]\n",
        "}\n",
        "\n",
        "# set the number of random combinations of hyper-parameter values that will be sampled\n",
        "# e.g., if 100 - then 100 MLPs will be trained and tested, using a random set of hyper-parameter\n",
        "# values each time\n",
        "num_itr = 100\n",
        "\n",
        "# run random search\n",
        "print(\"> STARTING RANDOM SEARCH ...\")\n",
        "start_time = time()\n",
        "\n",
        "# running random search using THE ORIGINAL feature values (X)\n",
        "results = run_random_search(model, X, y, param_dict, num_itr)\n",
        "\n",
        "end_time = time()\n",
        "print(\"> RANDOM SEARCH COMPLETE\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "          \" parameter settings.\" % ((end_time - start_time), num_itr))\n",
        "report(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrymlAy5lYIa",
        "outputId": "ff2f3aaf-5c7f-4cc8-d536-adf354e7e810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> STARTING RANDOM SEARCH ...\n",
            "> RANDOM SEARCH COMPLETE\n",
            "\n",
            "RandomizedSearchCV took 26.56 seconds for 100 candidates parameter settings.\n",
            "\n",
            "Models ranked according to score\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.99 (+/- 0.02)\n",
            "Params: {'max_iter': 1000, 'learning_rate': 'constant', 'hidden_layer_sizes': 50, 'activation': 'relu'}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.99 (+/- 0.03)\n",
            "Params: {'max_iter': 2000, 'learning_rate': 'adaptive', 'hidden_layer_sizes': 20, 'activation': 'logistic'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.03)\n",
            "Params: {'max_iter': 500, 'learning_rate': 'adaptive', 'hidden_layer_sizes': 50, 'activation': 'relu'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.04)\n",
            "Params: {'max_iter': 500, 'learning_rate': 'invscaling', 'hidden_layer_sizes': 10, 'activation': 'tanh'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.03)\n",
            "Params: {'max_iter': 2000, 'learning_rate': 'constant', 'hidden_layer_sizes': 20, 'activation': 'tanh'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.03)\n",
            "Params: {'max_iter': 1000, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (20, 20), 'activation': 'logistic'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.03)\n",
            "Params: {'max_iter': 2000, 'learning_rate': 'adaptive', 'hidden_layer_sizes': (20, 20), 'activation': 'tanh'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.03)\n",
            "Params: {'max_iter': 500, 'learning_rate': 'adaptive', 'hidden_layer_sizes': 50, 'activation': 'identity'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.04)\n",
            "Params: {'max_iter': 2000, 'learning_rate': 'constant', 'hidden_layer_sizes': 5, 'activation': 'tanh'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.07)\n",
            "Params: {'max_iter': 500, 'learning_rate': 'constant', 'hidden_layer_sizes': (10, 10), 'activation': 'identity'}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.98 (+/- 0.03)\n",
            "Params: {'max_iter': 1000, 'learning_rate': 'constant', 'hidden_layer_sizes': (10, 10), 'activation': 'logistic'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from time import time\n",
        "\n",
        "# instantiating the model (not setting any hyper-parameters here)\n",
        "model = model = MLPClassifier()\n",
        "\n",
        "# specify parameters and potential values to sample from\n",
        "param_dict = {\n",
        "    \"hidden_layer_sizes\": [5, 10, 20, 30, 50, 100, (5,5), (10,10), (20,20)],\n",
        "    \"max_iter\": [100, 300, 500, 1000, 2000],\n",
        "    \"activation\": [\"tanh\", \"relu\", \"logistic\", \"identity\"],\n",
        "    \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"]\n",
        "}\n",
        "\n",
        "# set the number of random combinations of hyper-parameter values that will be sampled\n",
        "# e.g., if 100 - then 100 MLPs will be trained and tested, using a random set of hyper-parameter\n",
        "# values each time\n",
        "num_itr = 100\n",
        "\n",
        "# run random search\n",
        "print(\"> STARTING RANDOM SEARCH ...\")\n",
        "start_time = time()\n",
        "\n",
        "# running random search using THE SCALED feature values (X_scaled)\n",
        "results = run_random_search(model, X_scaled, y, param_dict, num_itr)\n",
        "\n",
        "end_time = time()\n",
        "print(\"> RANDOM SEARCH COMPLETE\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "          \" parameter settings.\" % ((end_time - start_time), num_itr))\n",
        "report(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NSjaAl2lYIa"
      },
      "source": [
        "### Final remarks\n",
        "\n",
        "You should see several key things above:\n",
        "\n",
        "* The maximum performance on the original dataset / feature values capped at 96% accuracy, which is much higher than the manual tweaking done above.\n",
        "* However, using the scaled feature values allowed a 99% accuracy, which is really high!\n",
        "* There are different combinations of hyper-parameters that lead to the same accuracy. Which is why using a hyper-parameter optimisation function like Random Search is desired, because it would take a long time to manually try out the same number of combinations as we could do in 20-30 seconds here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k3k3yRFlYIa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}